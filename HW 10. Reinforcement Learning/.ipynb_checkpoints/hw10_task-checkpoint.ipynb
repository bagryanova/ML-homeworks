{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tbfD0MQQ9VZ"
   },
   "source": [
    "# Grid World\n",
    "__Суммарное количество баллов: 10__\n",
    "\n",
    "__Решение отправлять на `ml.course.practice@gmail.com`__\n",
    "\n",
    "__Тема письма: `[HSE][ML][HW10] <ФИ>`, где вместо `<ФИ>` указаны фамилия и имя__\n",
    "\n",
    "В этом задании вам предстоит реализовать несколько алгоритмов классического обучения с подкреплением для решения нескольких вариантов задачи GridWorld. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wM07PGqvRCA9"
   },
   "outputs": [],
   "source": [
    "!pip install gym[all] pyvirtualdisplay > /dev/null 2>&1\n",
    "!pip install tqdm > /dev/null 2>&1\n",
    "!pip install pillow\n",
    "!apt update > /dev/null 2>&1\n",
    "!apt install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "!wget https://noto-website-2.storage.googleapis.com/pkgs/NotoMono-hinted.zip > /dev/null 2>&1\n",
    "!unzip NotoMono-hinted.zip > /dev/null 2>&1\n",
    "!mv NotoMono-Regular.ttf /usr/share/fonts/truetype/ > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFmL2yATQ9Va"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4UJmkUsQQ9Vh"
   },
   "source": [
    "### Grid World\n",
    "В этом задании Вам придется работать со средой Grid World. Мир представляет из себя ограниченную сетку, по которой может перемещаться агент. Некоторые клетки могут быть заблокированы, другие будут приводить к завершению путешествия агента с различным исходом.\n",
    "\n",
    "#### Взаимодействие со средой\n",
    "`get_actions(state)` возвращает список доступных в состонии `state` действий.\n",
    "\n",
    "`get_states()` возвращает все состояния, в которых когда-либо может быть агент.\n",
    "\n",
    "`get_transition(state, action)` возвращает состояние, в котором окажется агент если совершит действие `action` в состоянии `state`.\n",
    "\n",
    "`get_reward(state, action)` возвращает награду, которую получит агент если совершит действие `action` в состоянии `state`.\n",
    "\n",
    "`step(action)` - метод, позволяющий совершить действие в среде. На вход принимает действия из набора доступных действий, на выход возвращает `state, reward, done`. Награда является вещественным числом, done может принимать значения `True` или `False`\n",
    "\n",
    "`reset()` - метод, позволяющий вернуть среду к изначальному состоянию. Обычно используется после того, как done принял значение `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YnE0UsoQ9Vi"
   },
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, grid):\n",
    "        self._height = len(grid)\n",
    "        self._width = len(grid[0])\n",
    "        self._max_reward = -1e9\n",
    "        self._min_reward = 1e9\n",
    "        for i in range(self._height):\n",
    "            for j in range(self._width):\n",
    "                if grid[i][j] == 'S':\n",
    "                    self._start_state = (i, j)\n",
    "                if not isinstance(grid[i][j], str):\n",
    "                    self._max_reward = max(grid[i][j], self._max_reward)\n",
    "                    self._min_reward = min(grid[i][j], self._min_reward)\n",
    "        self._state = self._start_state\n",
    "        self._grid = grid\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = self.get_reward(self._state, action)\n",
    "        next_state = self.get_transition(self._state, action)\n",
    "        self._state = next_state\n",
    "        done = next_state[0] < 0 or next_state[1] < 0\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def reset(self):\n",
    "        self._state = self._start_state\n",
    "        return self._state\n",
    "    \n",
    "    def _get_info(self, x, y):\n",
    "        reward = self._grid[y][x]\n",
    "        if isinstance(reward, str):\n",
    "            reward = 0.0\n",
    "            is_final = False\n",
    "        else:\n",
    "            is_final = True\n",
    "        is_block = self._grid[y][x] == \"#\"\n",
    "        return reward, is_final, is_block\n",
    "    \n",
    "    def get_actions(self, state):\n",
    "        return \"left\", \"right\", \"top\", \"bottom\"\n",
    "    \n",
    "    def get_states(self):\n",
    "        states = []\n",
    "        for i in range(self._height):\n",
    "            for j in range(self._width):\n",
    "                if self._grid[i][j] != '#':\n",
    "                    states.append((i, j))\n",
    "        return tuple(states)\n",
    "    \n",
    "    def get_transition(self, state, action):\n",
    "        y, x = state\n",
    "        if (y < 0) or (x < 0):\n",
    "            return state\n",
    "        if not isinstance(self._grid[y][x], str):\n",
    "            return -1, -1\n",
    "        if self._grid[y][x] == '#':\n",
    "            return state\n",
    "        if action == 'left' and x > 0 and self._grid[y][x - 1] != '#':\n",
    "            return y, x - 1\n",
    "        if action == 'right' and x < self._width - 1 and self._grid[y][x + 1] != '#':\n",
    "            return y, x + 1\n",
    "        if action == 'top' and y > 0 and self._grid[y - 1][x] != '#':\n",
    "            return y - 1, x\n",
    "        if action == 'bottom' and y < self._height - 1 and self._grid[y + 1][x] != '#':\n",
    "            return y + 1, x\n",
    "        return state\n",
    "    \n",
    "    def get_reward(self, state, action):\n",
    "        next_state = self.get_transition(state, action)\n",
    "        if state[0] < 0 or state[1] < 0:\n",
    "            return 0.0\n",
    "        if next_state[0] == state[0] and next_state[1] == state[1]:\n",
    "            return 0.0\n",
    "        if isinstance(self._grid[state[0]][state[1]], str):\n",
    "            return 0.0\n",
    "        return self._grid[state[0]][state[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38o7jBGRQ9Vo"
   },
   "source": [
    "### ТЕХНИЧЕСКАЯ ЧАСТЬ\n",
    "Тут лежит код для отображения картинок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhW-m4iYQ9Vp"
   },
   "outputs": [],
   "source": [
    "class GridworldRanderer:\n",
    "    def __init__(self, gridworld, cell_px=128, border_px=32, font_size=18):\n",
    "        self.gridworld = gridworld\n",
    "        self.cell_px = cell_px\n",
    "        self.width = self.gridworld._width\n",
    "        self.height = self.gridworld._height\n",
    "        self.border_px = border_px \n",
    "        self.image = Image.new('RGB', (self.gridworld._width * cell_px + 2 * border_px,\n",
    "                                      self.gridworld._height * cell_px + 2 * border_px), \n",
    "                              (0, 0, 0))\n",
    "        self.drawer = ImageDraw.Draw(self.image)\n",
    "        self.font = ImageFont.truetype(\"NotoMono-Regular.ttf\", font_size)\n",
    "        \n",
    "    def _render_borders(self, is_q=False):\n",
    "        self.drawer.line([(self.border_px, self.border_px), \n",
    "                          (self.border_px + self.width * self.cell_px, self.border_px), \n",
    "                          (self.border_px + self.width * self.cell_px, self.border_px + self.height * self.cell_px), \n",
    "                          (self.border_px, self.border_px + self.height * self.cell_px), \n",
    "                          (self.border_px, self.border_px)], fill=(255, 255, 255), width=6)\n",
    "        for i in range(self.width - 1):\n",
    "            self.drawer.line([(self.border_px + (i + 1) * self.cell_px, self.border_px), \n",
    "                              (self.border_px + (i + 1) * self.cell_px, self.border_px + self.height * self.cell_px)], \n",
    "                             fill=(255, 255, 255), width=4)\n",
    "        for i in range(self.height - 1):\n",
    "            self.drawer.line([(self.border_px, self.border_px + (i + 1) * self.cell_px), \n",
    "                              (self.border_px + self.width * self.cell_px, self.border_px + (i + 1) * self.cell_px)], \n",
    "                             fill=(255, 255, 255), width=4)\n",
    "        for i in range(self.width):\n",
    "            for j in range(self.height):\n",
    "                reward, is_final, is_block = self.gridworld._get_info(i, j)\n",
    "                if is_final:\n",
    "                    self.drawer.line([(self.border_px + (i) * self.cell_px + 8, self.border_px + (j) * self.cell_px + 8), \n",
    "                                      (self.border_px + (i + 1) * self.cell_px - 8, self.border_px + (j) * self.cell_px + 8), \n",
    "                                      (self.border_px + (i + 1) * self.cell_px - 8, self.border_px + (j + 1) * self.cell_px - 8), \n",
    "                                      (self.border_px + (i) * self.cell_px + 8, self.border_px + (j + 1) * self.cell_px - 8), \n",
    "                                      (self.border_px + (i) * self.cell_px + 8, self.border_px + (j) * self.cell_px + 8)], fill=(255, 255, 255), width=2)\n",
    "                elif is_q and not is_block:\n",
    "                    self.drawer.line([\n",
    "                        (self.border_px + i * self.cell_px, self.border_px + j * self.cell_px),\n",
    "                        (self.border_px + (i + 1) * self.cell_px, self.border_px + (j + 1) * self.cell_px)], \n",
    "                        fill=(255, 255, 255), width=2)\n",
    "                    self.drawer.line([\n",
    "                        (self.border_px + i * self.cell_px, self.border_px + (j + 1) * self.cell_px),\n",
    "                        (self.border_px + (i + 1) * self.cell_px, self.border_px + j * self.cell_px)], \n",
    "                        fill=(255, 255, 255), width=2)\n",
    "        \n",
    "    def _clear(self):\n",
    "        self.image = Image.new('RGB', (self.gridworld._width * self.cell_px + 2 * self.border_px,\n",
    "                                      self.gridworld._height * self.cell_px + 2 * self.border_px), \n",
    "                              (0, 0, 0))\n",
    "        self.drawer = ImageDraw.Draw(self.image)\n",
    "        self.drawer.font = self.font\n",
    "    \n",
    "    def _render_value_squere(self, x, y, v, min_v, max_v, is_final):\n",
    "        if v == 0:\n",
    "            color = (0, 0, 0)\n",
    "        elif v > 0:\n",
    "            color = (0, int(255 * min(abs(v) / (abs(max_v) + 1e-8), 1.0)), 0)\n",
    "        else:\n",
    "            color = (int(255 * min(abs(v) / (abs(min_v) + 1e-8), 1.0)), 0, 0)\n",
    "        self.drawer.rectangle([self.border_px + x * self.cell_px, self.border_px + y * self.cell_px,\n",
    "                              self.border_px + (x + 1) * self.cell_px, self.border_px + (y + 1) * self.cell_px],\n",
    "                             fill=color)\n",
    "        s = f\"{v:.2f}\"\n",
    "        tx, ty = self.drawer.textsize(s)\n",
    "        self.drawer.text((\n",
    "            self.border_px + x * self.cell_px + self.cell_px // 2 - tx // 2,\n",
    "            self.border_px + y * self.cell_px + self.cell_px // 2 - ty // 2\n",
    "        ), s, fill=(255, 255, 255), stroke_width=1, stroke_fill=(0, 0, 0))\n",
    "    \n",
    "    def _render_q_squere(self, x, y, qs, min_v, max_v, is_final):\n",
    "        actions = [(\"top\", 0), (\"right\", 1), (\"bottom\", 2), (\"left\", 3)]\n",
    "        for k, a in actions:\n",
    "            v = qs[k]\n",
    "            if v == 0:\n",
    "                color = (0, 0, 0)\n",
    "            elif v > 0:\n",
    "                color = (0, int(255 * min(abs(v) / (abs(max_v) + 1e-8), 1.0)), 0)\n",
    "            else:\n",
    "                color = (int(255 * min(abs(v) / (abs(min_v) + 1e-8), 1.0)), 0 , 0)\n",
    "            self.drawer.polygon([\n",
    "                self.border_px + (x + (a + 1) // 2 % 2) * self.cell_px, self.border_px + (y + (a + 1) // 2 % 2) * self.cell_px,\n",
    "                self.border_px + x * self.cell_px + self.cell_px // 2, self.border_px + y * self.cell_px  + self.cell_px // 2,\n",
    "                self.border_px + (x + (a // 2 + 1) % 2) * self.cell_px, self.border_px + (y + (a // 2) % 2) * self.cell_px\n",
    "            ], fill=color)\n",
    "            dx = 0.5 + 0.25 * (a % 2) * (1 - 2 * (a // 2))\n",
    "            dy = 0.5 + 0.25 * ((a + 1) % 2) * (2 * (a // 2) - 1)\n",
    "            tpx = self.border_px + int((x + dx) * self.cell_px)\n",
    "            tpy = self.border_px + int((y + dy) * self.cell_px)\n",
    "            s = f\"{v:.2f}\"\n",
    "            tx, ty = self.drawer.textsize(s)\n",
    "            self.drawer.text((tpx - tx // 2, tpy - ty // 2), s, \n",
    "                             fill=(255, 255, 255), stroke_width=0, stroke_fill=(0, 0, 0))\n",
    "            \n",
    "    def render_rewards(self):\n",
    "        self._clear()\n",
    "        max_v = self.gridworld._max_reward\n",
    "        min_v = self.gridworld._min_reward\n",
    "        for i in range(self.width):\n",
    "            for j in range(self.height):\n",
    "                reward, is_final, is_block = self.gridworld._get_info(i, j)\n",
    "                if is_block: \n",
    "                    continue\n",
    "                self._render_value_squere(i, j, reward, min_v, max_v, is_final)\n",
    "        self._render_borders()\n",
    "        return np.array(self.image)\n",
    "    \n",
    "    def render_value(self, agent):\n",
    "        self._clear()\n",
    "        max_v = self.gridworld._max_reward\n",
    "        min_v = self.gridworld._min_reward\n",
    "        for i in range(self.width):\n",
    "            for j in range(self.height):\n",
    "                reward, is_final, is_block = self.gridworld._get_info(i, j)\n",
    "                if is_block: \n",
    "                    continue\n",
    "                if is_final:\n",
    "                    self._render_value_squere(i, j, reward, min_v, max_v, is_final)\n",
    "                else:\n",
    "                    value = agent.get_value((j, i))\n",
    "                    self._render_value_squere(i, j, value, min_v, max_v, is_final)\n",
    "        self._render_borders()\n",
    "        return np.array(self.image)\n",
    "    \n",
    "    def render_q(self, agent):\n",
    "        self._clear()\n",
    "        max_v = self.gridworld._max_reward\n",
    "        min_v = self.gridworld._min_reward\n",
    "        for i in range(self.width):\n",
    "            for j in range(self.height):\n",
    "                reward, is_final, is_block = self.gridworld._get_info(i, j)\n",
    "                if is_block: \n",
    "                    continue\n",
    "                if is_final:\n",
    "                    self._render_value_squere(i, j, reward, min_v, max_v, is_final)\n",
    "                else:\n",
    "                    qs = {}\n",
    "                    for a in self.gridworld.get_actions((i, j)):\n",
    "                        qs[a] = agent.get_q((j, i), a)\n",
    "                    self._render_q_squere(i, j,qs, min_v, max_v, is_final)\n",
    "        self._render_borders(True)\n",
    "        return np.array(self.image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiWd1qmfQ9Vu"
   },
   "outputs": [],
   "source": [
    "def _show_image(img, title=None, figsize=(9, 9)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "def visualise(gridworld, agent):\n",
    "    randerer = GridworldRanderer(gridworld)\n",
    "    _show_image(randerer.render_rewards(), \"Environment\")\n",
    "    _show_image(randerer.render_value(agent), \"Value function\")\n",
    "    _show_image(randerer.render_q(agent), \"Q function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GivrlSPTQ9V0"
   },
   "source": [
    "### Различные миры\n",
    "В этой части определены различные миры. Каждый из них так или иначе раскрывает возможные проблемы алгоритмов обучения с подкреплением."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfhD8vOyQ9V0"
   },
   "outputs": [],
   "source": [
    "classic_grid = [\n",
    "    [\" \", \" \", \" \", 1.0],\n",
    "    [\" \", \"#\", \" \", -1.0],\n",
    "    [\"S\", \" \", \" \", \" \"]\n",
    "]\n",
    "exploration_grid = [\n",
    "    [\" \", \" \", \" \", 1.0],\n",
    "    [\" \", \"#\", \" \", 2.0],\n",
    "    [\"S\", \" \", \" \", \" \"]\n",
    "]\n",
    "cliff_grid = [\n",
    "    [\" \", \" \", \" \", \" \", \" \"],\n",
    "    [\" \", \"#\", \"#\", \"#\", \" \"],\n",
    "    [\"S\", \" \", \" \", \" \", 1.0],\n",
    "    [-1.0, -1.0, -1.0, -1.0, -1.0]\n",
    "]\n",
    "cliff_exploration_grid = [\n",
    "    [\" \", \" \", \" \", \" \", \" \", \" \", \" \"],\n",
    "    [\" \", \"#\", \"#\", \"#\", \"#\", \"#\", \" \"],\n",
    "    [\" \", \" \", \" \", \" \", \" \", 1.0, \" \"],\n",
    "    [\" \", \"#\", \"#\", \"#\", \"#\", \"#\", 8.0],\n",
    "    [\"S\", \" \", \" \", \" \", \" \", \" \", 1.0],\n",
    "    [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]\n",
    "]\n",
    "hard_exploration_grid = [\n",
    "    [\" \", \" \", \" \", \" \", \" \", \" \", \" \"],\n",
    "    [\" \", \"#\", \"#\", \"#\", \"#\", \"#\", \" \"],\n",
    "    [\" \", \" \", \" \", \" \", \" \", 3.0, \" \"],\n",
    "    [\" \", \"#\", \"#\", \"#\", \"#\", \"#\", \" \"],\n",
    "    [\" \", \" \", \" \", \" \", \" \", 2.0, \" \"],\n",
    "    [\" \", \"#\", \"#\", \"#\", \"#\", \"#\", \" \"],\n",
    "    [\"S\", \" \", \" \", \" \", \" \", 1.0, 10.0],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUeFSGMuQ9V5"
   },
   "source": [
    "### Общий интерфейс для агента\n",
    "`get_action(state)` возвращает состояние, которое хочет совершить агент в состоянии `state`\n",
    "\n",
    "`get_value(state)` возвращает значение Value-function для состояния `state`\n",
    "\n",
    "`get_q(state, action)` возвращает значение Q-function для состояния `state` и действия `action`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vu-glLLQ9V6"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def get_action(self, state):\n",
    "        pass\n",
    "    \n",
    "    def get_value(self, state):\n",
    "        pass\n",
    "    \n",
    "    def get_q(self, state, action):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLRW3bz8Q9WA"
   },
   "source": [
    "### Задание 1 (4 балла)\n",
    "Реализуйте алгоритм value iteration. Обучение должно проходить в методе `__init__` в течении `iterations` итераций. Каждая итерация включает в себя проход по всем возможным состояниям среды. Во время обучения нужно учитывать learning rate `alpha` и коэффициент дисконтирования `gamma`.\n",
    "\n",
    "_Hint:_ используйте методы `gridworld.get_reward`, `gridworld.get_transition`, `gridworld.get_actions` и `gridworld.get_states`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XC0xhYjNQ9WC"
   },
   "outputs": [],
   "source": [
    "class ValueIteration(Agent):\n",
    "    def __init__(self, gridworld, iterations=100, alpha=0.9, gamma=0.8):\n",
    "        self.gamma = gamma\n",
    "        self.gridworld = gridworld\n",
    "        for i in range(iterations):\n",
    "            pass # TODO: обновить приближение value function\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        # TODO: реализовать выбор лучшего действия\n",
    "        return None\n",
    "    \n",
    "    def get_value(self, state):\n",
    "        # TODO: реализовать подсчет value function для state\n",
    "        return None\n",
    "    \n",
    "    def get_q(self, state, action):\n",
    "        # TODO: реализовать подсчет q-function для пары (state, action)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGXcr7FYQ9WH"
   },
   "source": [
    "#### Посмотрим, что получилось\n",
    "Для проверки алгоритма можно использовать различные варианты среды, полный список находится выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6apoCM1xQ9WJ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gridworld = GridWorld(hard_exploration_grid)\n",
    "agent = ValueIteration(gridworld)\n",
    "visualise(gridworld, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OKkLQjbQ9WQ"
   },
   "source": [
    "### Задание 2 (4 балла)\n",
    "Реализуйте алгоритм Q-learning. Процесс обучения реализован вне класса, поэтому достаточно реализовать метод `update` для пересчета значений Q-function. Во время обнвления нужно учитывать learning rate `alpha` и коэффициент дисконтирования `gamma`.\n",
    "\n",
    "В этом задании использовать `gridworld.get_reward`, `gridworld.get_transition` и `gridworld.get_states` нельзя (`gridworld.get_actions` можно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwzSNL6kQ9WR"
   },
   "outputs": [],
   "source": [
    "class QLearning(Agent):\n",
    "    def __init__(self, gridworld, alpha=0.9, gamma=0.8):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.gridworld = gridworld\n",
    "\n",
    "    def update(self, state, action, next_state, reward, done):\n",
    "        pass # TODO: реализовать обновление значений q-function\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        # TODO: реализовать выбор лучшего действия\n",
    "        return None \n",
    "    \n",
    "    def get_value(self, state):\n",
    "        # TODO: реализовать подсчет value function для state\n",
    "        return None\n",
    "    \n",
    "    def get_q(self, state, action):\n",
    "        # TODO: реализовать подсчет q-function для пары (state, action)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdK9OPu6Q9WW"
   },
   "source": [
    "Имплементация epsilon-greedy алгоритма. Просто добавляем случайные действия с вероятностью epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-slP7FjQ9WX"
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedy(QLearning):\n",
    "    def __init__(self, gridworld, alpha=0.5, gamma=0.9, eps=0.1):\n",
    "        self.eps = eps\n",
    "        super().__init__(gridworld, alpha=alpha, gamma=gamma)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if self.eps > random.random():\n",
    "            return random.choice(gridworld.get_actions(state))\n",
    "        else:\n",
    "            return super().get_action(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KE55JmxnQ9Wd"
   },
   "source": [
    "Процесс обучения Q-learning. Сначала обучаем агента в течении `timesteps` шагов среды, после чего тестируем его на `test_episodes` эпизодах с ограничением не более `test_max_steps` шагов на каждый. Возвращает количство завершившихся эпизодов во время тестирвания и сумму полученных наград."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqUxx3pLQ9Wd"
   },
   "outputs": [],
   "source": [
    "def train(gridworld, agent, train_method, timesteps, test_episodes=100, test_max_steps=100):\n",
    "    train_method(gridworld, agent, timesteps)\n",
    "    time = 0\n",
    "    done_count = 0\n",
    "    sum_reward = 0\n",
    "    for ep in range(test_episodes):\n",
    "        done = False\n",
    "        sr = 0.\n",
    "        state = gridworld.reset()\n",
    "        for time in range(test_max_steps):\n",
    "            action = agent.get_action(state)\n",
    "            state, reward, done = gridworld.step(action)\n",
    "            sr += reward\n",
    "            if done:\n",
    "                done_count += 1\n",
    "                sum_reward += sr\n",
    "                break\n",
    "    return done_count, sum_reward\n",
    "\n",
    "def train_qlearning(gridworld, agent, timesteps):\n",
    "    done = False\n",
    "    state = gridworld.reset()\n",
    "    for t in range(timesteps):\n",
    "        if done:\n",
    "            state = gridworld.reset()\n",
    "            done = False\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = gridworld.step(action)\n",
    "        agent.update(state, action, next_state, reward, done)\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfPxa_WTQ9Wi"
   },
   "source": [
    "Посмотрим на зависимость результата от $\\epsilon$. Можно посмотреть также и на разные среды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8DizJkhQ9Wi"
   },
   "outputs": [],
   "source": [
    "gridworld = GridWorld(classic_grid)\n",
    "for eps in (0.0, 0.1, 0.25, 0.5, 0.75):\n",
    "    agent = EpsilonGreedy(gridworld, eps=eps)\n",
    "    done_count, sum_reward = train(gridworld, agent, train_qlearning, 1000000)\n",
    "    if done_count != 0:\n",
    "        sum_reward /= done_count\n",
    "    print(f\"Epsilon: {eps} | Done count: {done_count} | Avg reward {sum_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n65ho5W6U2s8"
   },
   "outputs": [],
   "source": [
    "gridworld = GridWorld(classic_grid)\n",
    "agent = EpsilonGreedy(gridworld, eps=0.2)\n",
    "done_count, sum_reward = train(gridworld, agent, train_qlearning, 1000000)\n",
    "if done_count != 0:\n",
    "    sum_reward /= done_count\n",
    "print(f\"Done count: {done_count} | Avg reward {sum_reward}\")\n",
    "visualise(gridworld, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtVkGLbOW2Dx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQSF3p7tQ9Wn"
   },
   "outputs": [],
   "source": [
    "gridworld = GridWorld(cliff_grid)\n",
    "for eps in (0.0, 0.1, 0.25, 0.5, 0.75):\n",
    "    agent = EpsilonGreedy(gridworld, eps=eps)\n",
    "    done_count, sum_reward = train(gridworld, agent, train_qlearning, 1000000)\n",
    "    if done_count != 0:\n",
    "        sum_reward /= done_count\n",
    "    print(f\"Epsilon: {eps} | Done count: {done_count} | Avg reward {sum_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SE8VJ_5UlXb"
   },
   "outputs": [],
   "source": [
    "gridworld = GridWorld(cliff_grid)\n",
    "agent = EpsilonGreedy(gridworld, eps=0.2)\n",
    "done_count, sum_reward = train(gridworld, agent, train_qlearning, 1000000)\n",
    "if done_count != 0:\n",
    "    sum_reward /= done_count\n",
    "print(f\"Done count: {done_count} | Avg reward {sum_reward}\")\n",
    "visualise(gridworld, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iHWtJnHQ9Wq"
   },
   "source": [
    "Кажется, что в Cliff World у Q-learning действительно возникли проблемы, поэтому\n",
    "\n",
    "### Задание 3 (2 балла)\n",
    "Реализуйте алгоритм SARSA. Для этого Вам придется изменить метод `update`, а также реализовать процедуру `train_sarsa` аналогично процедуре `train_qlearning`.\n",
    "\n",
    "В этом задании использовать `gridworld.get_reward`, `gridworld.get_transition` и `gridworld.get_states` нельзя (`gridworld.get_actions` можно)\n",
    "\n",
    "_Hint:_ сейчас становится критичным подбор learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uV4CClwdQ9Wq"
   },
   "outputs": [],
   "source": [
    "class SARSA(EpsilonGreedy):\n",
    "    def update(self, state, action, next_state, next_action, reward, done):\n",
    "        pass # TODO: implement\n",
    "\n",
    "def train_sarsa(gridworld, agent, timesteps):\n",
    "    # TODO: Это просто копипаста метода train_qlearning. Его нужно адаптировать для обучения алгоритма SARSA\n",
    "    done = False\n",
    "    state = gridworld.reset()\n",
    "    for t in range(timesteps):\n",
    "        if done:\n",
    "            state = gridworld.reset()\n",
    "            done = False\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = gridworld.step(action)\n",
    "        agent.update(state, action, next_state, reward, done)\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opkXqz4lQ9Wt"
   },
   "outputs": [],
   "source": [
    "gridworld = GridWorld(cliff_grid)\n",
    "for eps in (0.0, 0.1, 0.25, 0.5, 0.75):\n",
    "    agent = SARSA(gridworld, eps=eps, alpha=0.2)\n",
    "    done_count, sum_reward = train(gridworld, agent, train_sarsa, 1000000)\n",
    "    if done_count != 0:\n",
    "        sum_reward /= done_count\n",
    "    print(f\"Epsilon: {eps} | Done count: {done_count} | Avg reward {sum_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KASaE2TiT8-Y"
   },
   "outputs": [],
   "source": [
    "gridworld = GridWorld(cliff_grid)\n",
    "agent = SARSA(gridworld, eps=0.2, alpha=0.2)\n",
    "done_count, sum_reward = train(gridworld, agent, train_sarsa, 1000000)\n",
    "if done_count != 0:\n",
    "    sum_reward /= done_count\n",
    "print(f\"Done count: {done_count} | Avg reward {sum_reward}\")\n",
    "visualise(gridworld, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-uJVaMCBQ9Ww"
   },
   "outputs": [],
   "source": [
    "gridworld = GridWorld(cliff_exploration_grid)\n",
    "for eps in (0.1, 0.2, 0.3, 0.5, 0.7):\n",
    "    agent = SARSA(gridworld, eps=eps, alpha=0.2)\n",
    "    done_count, sum_reward = train(gridworld, agent, train_sarsa, 1000000)\n",
    "    if done_count != 0:\n",
    "        sum_reward /= done_count\n",
    "    print(f\"Epsilon: {eps} | Done count: {done_count} | Avg reward {sum_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxncZsjVW-CU"
   },
   "outputs": [],
   "source": [
    "gridworld = GridWorld(cliff_exploration_grid)\n",
    "agent = SARSA(gridworld, eps=0.2, alpha=0.2)\n",
    "done_count, sum_reward = train(gridworld, agent, train_sarsa, 1000000)\n",
    "if done_count != 0:\n",
    "    sum_reward /= done_count\n",
    "print(f\"Done count: {done_count} | Avg reward {sum_reward}\")\n",
    "visualise(gridworld, agent)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4UJmkUsQQ9Vh",
    "38o7jBGRQ9Vo",
    "GivrlSPTQ9V0"
   ],
   "name": "hw12_task",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
