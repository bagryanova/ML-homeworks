{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owQHb50qJY1m"
   },
   "source": [
    "# Случайные леса\n",
    "__Суммарное количество баллов: 10__\n",
    "\n",
    "__Решение отправлять на `ml.course.practice@gmail.com`__\n",
    "\n",
    "__Тема письма: `[HSE][ML][HW09] <ФИ>`, где вместо `<ФИ>` указаны фамилия и имя__\n",
    "\n",
    "В этом задании вам предстоит реализовать ансамбль деревьев решений, известный как случайный лес, применить его к публичным данным пользователей социальной сети Вконтакте, и сравнить его эффективность с ансамблем, предоставляемым библиотекой CatBoost.\n",
    "\n",
    "В результате мы сможем определить, какие подписки пользователей больше всего влияют на определение возраста и пола человека. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cBlwWMWbJY1q"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in /Users/macbook/opt/anaconda3/lib/python3.8/site-packages (1.0.3)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /Users/macbook/opt/anaconda3/lib/python3.8/site-packages (from catboost) (1.2.4)\n",
      "Requirement already satisfied: plotly in /Users/macbook/opt/anaconda3/lib/python3.8/site-packages (from catboost) (5.4.0)\n",
      "Requirement already satisfied: six in /Users/macbook/opt/anaconda3/lib/python3.8/site-packages (from catboost) (1.15.0)\n",
      "Requirement already satisfied: matplotlib in /Users/macbook/opt/anaconda3/lib/python3.8/site-packages (from catboost) (3.3.4)\n",
      "Requirement already satisfied: scipy in /Users/macbook/opt/anaconda3/lib/python3.8/site-packages (from catboost) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /Users/macbook/opt/anaconda3/lib/python3.8/site-packages (from catboost) (1.20.1)\n",
      "Requirement already satisfied: graphviz in /Users/macbook/opt/anaconda3/lib/python3.8/site-packages (from catboost) (0.19)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/macbook/opt/anaconda3/lib/python3.8/site-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/macbook/opt/anaconda3/lib/python3.8/site-packages (from pandas>=0.24.0->catboost) (2021.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/macbook/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/macbook/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->catboost) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/macbook/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->catboost) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/macbook/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->catboost) (8.2.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/macbook/opt/anaconda3/lib/python3.8/site-packages (from plotly->catboost) (8.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RRcsIXHXJY1r"
   },
   "outputs": [],
   "source": [
    "def gini(x):\n",
    "    _, counts = np.unique(x, return_counts=True)\n",
    "    proba = counts / len(x)\n",
    "    return np.sum(proba * (1 - proba))\n",
    "    \n",
    "def entropy(x):\n",
    "    _, counts = np.unique(x, return_counts=True)\n",
    "    proba = counts / len(x)\n",
    "    return -np.sum(proba * np.log2(proba))\n",
    "\n",
    "def gain(left_y, right_y, criterion):\n",
    "    y = np.concatenate((left_y, right_y))\n",
    "    return criterion(y) - (criterion(left_y) * len(left_y) + criterion(right_y) * len(right_y)) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1mmD9DIJY1r"
   },
   "source": [
    "### Задание 1 (2 балла)\n",
    "Random Forest состоит из деревьев решений. Каждое такое дерево строится на одной из выборок, полученных при помощи bagging. Элементы, которые не вошли в новую обучающую выборку, образуют out-of-bag выборку. Кроме того, в каждом узле дерева мы случайным образом выбираем набор из `max_features` и ищем признак для предиката разбиения только в этом наборе.\n",
    "\n",
    "Сегодня мы будем работать только с бинарными признаками, поэтому нет необходимости выбирать значение признака для разбиения.\n",
    "\n",
    "#### Методы\n",
    "`predict(X)` - возвращает предсказанные метки для элементов выборки `X`\n",
    "\n",
    "#### Параметры конструктора\n",
    "`X, y` - обучающая выборка и соответствующие ей метки классов. Из нее нужно получить выборку для построения дерева при помощи bagging. Out-of-bag выборку нужно запомнить, она понадобится потом.\n",
    "\n",
    "`criterion=\"gini\"` - задает критерий, который будет использоваться при построении дерева. Возможные значения: `\"gini\"`, `\"entropy\"`.\n",
    "\n",
    "`max_depth=None` - ограничение глубины дерева. Если `None` - глубина не ограничена\n",
    "\n",
    "`min_samples_leaf=1` - минимальное количество элементов в каждом листе дерева.\n",
    "\n",
    "`max_features=\"auto\"` - количество признаков, которые могут использоваться в узле. Если `\"auto\"` - равно `sqrt(X.shape[1])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Callable, Union, NoReturn, Optional, Dict, Any, List\n",
    "\n",
    "class DecisionTreeLeaf:\n",
    "    \"\"\"\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    y : Тип метки (напр., int или str)\n",
    "        Метка класса, который встречается чаще всего среди элементов листа дерева\n",
    "    \"\"\"\n",
    "    def __init__(self, sub_y):\n",
    "        self.y = Counter(sub_y).most_common(1)[0][0]\n",
    "        \n",
    "\n",
    "class DecisionTreeNode:\n",
    "    \"\"\"\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    split_dim : int\n",
    "        Измерение, по которому разбиваем выборку.\n",
    "    split_value : float\n",
    "        Значение, по которому разбираем выборку.\n",
    "    left : Union[DecisionTreeNode, DecisionTreeLeaf]\n",
    "        Поддерево, отвечающее за случай x[split_dim] < split_value.\n",
    "    right : Union[DecisionTreeNode, DecisionTreeLeaf]\n",
    "        Поддерево, отвечающее за случай x[split_dim] >= split_value. \n",
    "    \"\"\"\n",
    "    def __init__(self, split_dim: int, split_value: float, \n",
    "                 left: Union['DecisionTreeNode', DecisionTreeLeaf], \n",
    "                 right: Union['DecisionTreeNode', DecisionTreeLeaf]):\n",
    "        self.split_dim = split_dim\n",
    "        self.split_value = split_value\n",
    "        self.left = left\n",
    "        self.right = right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(feature_vector, target_vector, min_split, criterion=gini):\n",
    "    \n",
    "    s_ind = np.argsort(np.array(feature_vector))\n",
    "    x = np.array(feature_vector)[s_ind]\n",
    "    y = np.array(target_vector)[s_ind]\n",
    "\n",
    "\n",
    "    n = x.shape[0]\n",
    "\n",
    "    thresholds = np.unique(x)\n",
    "    if thresholds.shape[0] == 1:\n",
    "        return (None, None)\n",
    "    thresholds = (thresholds[:-1] + thresholds[1:]) / 2\n",
    "    left, right = None, None\n",
    "    ind = 0\n",
    "    best_gain, best_threshold = None, None\n",
    "    tmp = []\n",
    "    for threshold in thresholds:\n",
    "        while x[ind] < threshold and ind < n:\n",
    "            ind += 1\n",
    "        if ind < min_split:\n",
    "            continue\n",
    "        if n - ind < min_split:\n",
    "            break\n",
    "        left = y[:ind]\n",
    "        right = y[ind:]\n",
    "        ig = gain(left, right, criterion)\n",
    "        tmp.append(ig)\n",
    "        if best_gain is None or ig > best_gain:\n",
    "            best_gain = ig\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold, best_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Vq5JcBJ4JY1s"
   },
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, X, y, criterion=\"gini\", max_depth=None, min_samples_leaf=1, max_features=\"auto\"):\n",
    "        n = X.shape[0]\n",
    "        indexes = np.random.randint(0, n, n)\n",
    "        self.X = X[indexes]\n",
    "        self.y = y[indexes]\n",
    "        out_of_bag_ind = [i not in set(indexes) for i in range(n)]\n",
    "        self.out_of_bag_x = X[out_of_bag_ind]\n",
    "        self.out_of_bag_y = y[out_of_bag_ind]\n",
    "        self.criterion = gini\n",
    "        if criterion == \"entropy\":\n",
    "            self.criterion = entropy\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = int(np.sqrt(X.shape[1]))\n",
    "        if max_features != \"auto\":\n",
    "            self.max_features = max_features\n",
    "        self.root = self._fit_node(self.X, self.y, 0)\n",
    "    \n",
    "    def out_of_bag(self):\n",
    "        return [self.out_of_bag_x, self.out_of_bag_y]\n",
    "            \n",
    "    def _fit_node(self, sub_X, sub_y, depth):\n",
    "        if np.all(sub_y == sub_y[0]):\n",
    "            node = DecisionTreeLeaf(sub_y)\n",
    "            return node\n",
    "        \n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            node = DecisionTreeLeaf(sub_y)\n",
    "            return node\n",
    "        feature_best, threshold_best, gain_best, split = None, None, None, None\n",
    "        feature_indexes = np.random.randint(0, sub_X.shape[1], self.max_features)\n",
    "        for feature in feature_indexes:\n",
    "            feature_vector = sub_X[:, feature]\n",
    "            \n",
    "            threshold, ig = find_best_split(feature_vector, sub_y, self.min_samples_leaf, self.criterion)\n",
    "            if threshold is not None and (gain_best is None or ig > gain_best):\n",
    "                feature_best = feature\n",
    "                gain_best = ig\n",
    "                split = feature_vector < threshold\n",
    "                threshold_best = threshold\n",
    "        if feature_best is None:\n",
    "            node = DecisionTreeLeaf(sub_y)\n",
    "            return node\n",
    "        left = self._fit_node(sub_X[split], sub_y[split], depth + 1)\n",
    "        right = self._fit_node(sub_X[~split], sub_y[~split], depth + 1)\n",
    "        node = DecisionTreeNode(feature_best, threshold_best, left, right)\n",
    "        return node\n",
    "\n",
    "    def _predict_node(self, x, node):\n",
    "        if isinstance(node, DecisionTreeLeaf):\n",
    "            return node.y\n",
    "        \n",
    "        f = node.split_dim\n",
    "\n",
    "        if x[f] < node.split_value:\n",
    "            return self._predict_node(x, node.left)\n",
    "        else:\n",
    "            return self._predict_node(x, node.right)\n",
    "\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predicted = []\n",
    "        for x in X:\n",
    "            predicted.append(self._predict_node(x, self.root))\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fI8Wj8b7JY1t"
   },
   "source": [
    "### Задание 2 (2 балла)\n",
    "Теперь реализуем сам Random Forest. Идея очень простая: строим `n` деревьев, а затем берем модальное предсказание.\n",
    "\n",
    "#### Параметры конструктора\n",
    "`n_estimators` - количество используемых для предсказания деревьев.\n",
    "\n",
    "Остальное - параметры деревьев.\n",
    "\n",
    "#### Методы\n",
    "`fit(X, y)` - строит `n_estimators` деревьев по выборке `X`.\n",
    "\n",
    "`predict(X)` - для каждого элемента выборки `X` возвращает самый частый класс, который предсказывают для него деревья."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kLiXR7kxJY1t"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class RandomForestClassifier:\n",
    "    def __init__(self, criterion=\"gini\", max_depth=None, min_samples_leaf=1, max_features=\"auto\", n_estimators=10):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.n_estimators = n_estimators\n",
    "        \n",
    "    def out_of_bag(self):\n",
    "        oob = [tree.out_of_bag() for tree in self.trees]\n",
    "        return oob\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.trees = [DecisionTree(X, y, self.criterion, self.max_depth, self.min_samples_leaf, self.max_features) for _ in tqdm(range(self.n_estimators))]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        cur = []\n",
    "        for tree in self.trees:\n",
    "            cur.append(tree.predict(X))\n",
    "        result = []\n",
    "        cur = np.array(cur)\n",
    "        #print(cur)\n",
    "        for i in range(X.shape[0]):\n",
    "            ideas = cur[:, i]\n",
    "            values, counts = np.unique(ideas, return_counts=True)\n",
    "            ind = np.argmax(counts)\n",
    "            result.append(values[ind])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nN5QZ3bJY1u"
   },
   "source": [
    "### Задание 3 (2 балла)\n",
    "Часто хочется понимать, насколько большую роль играет тот или иной признак для предсказания класса объекта. Есть различные способы посчитать его важность. Один из простых способов сделать это для Random Forest - посчитать out-of-bag ошибку предсказания `err_oob`, а затем перемешать значения признака `j` и посчитать ее (`err_oob_j`) еще раз. Оценкой важности признака `j` для одного дерева будет разность `err_oob_j - err_oob`, важность для всего леса считается как среднее значение важности по деревьям.\n",
    "\n",
    "Реализуйте функцию `feature_importance`, которая принимает на вход Random Forest и возвращает массив, в котором содержится важность для каждого признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fr1vG6JLJY1v"
   },
   "outputs": [],
   "source": [
    "def feature_importance(rfc, sample=10):\n",
    "    out_of_bag = rfc.out_of_bag()\n",
    "    res = 0\n",
    "    for i in range(len(out_of_bag)):\n",
    "        pred_y = rfc.predict(out_of_bag[i][0])\n",
    "        res += np.mean(pred_y == out_of_bag[i][1])\n",
    "    result = []\n",
    "    for i in range(len(out_of_bag)):\n",
    "        x = np.array(out_of_bag[i][0])[::sample].copy()\n",
    "        for j in tqdm(range(x.shape[1])):\n",
    "            x = np.array(out_of_bag[i][0])[::sample].copy()\n",
    "            if i == 0:\n",
    "                result.append(res)\n",
    "            x_j = x[:, j]\n",
    "            random.shuffle(x_j)\n",
    "            x[:, j] = x_j\n",
    "            pred_y = rfc.predict(x)\n",
    "            err_oob_j = np.mean(pred_y == out_of_bag[i][1][::sample])\n",
    "            result[j] -= err_oob_j\n",
    "    return result\n",
    "\n",
    "def most_important_features(importance, names, k=20):\n",
    "    # Выводит названия k самых важных признаков\n",
    "    idicies = np.argsort(importance)[::-1][:k]\n",
    "    return np.array(names)[idicies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cATCqIVkJY1v"
   },
   "source": [
    "Наконец, пришло время протестировать наше дерево на простом синтетическом наборе данных. В результате точность должна быть примерно равна `1.0`, наибольшее значение важности должно быть у признака с индексом `4`, признаки с индексами `2` и `3`  должны быть одинаково важны, а остальные признаки - не важны совсем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEL4eiHmJY1w"
   },
   "outputs": [],
   "source": [
    "def synthetic_dataset(size):\n",
    "    X = [(np.random.randint(0, 2), np.random.randint(0, 2), i % 6 == 3, \n",
    "          i % 6 == 0, i % 3 == 2, np.random.randint(0, 2)) for i in range(size)]\n",
    "    y = [i % 3 for i in range(size)]\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = synthetic_dataset(1000)\n",
    "rfc = RandomForestClassifier(n_estimators=10)\n",
    "rfc.fit(X, y)\n",
    "pred = rfc.predict(X)\n",
    "#print(pred, y)\n",
    "print(\"Accuracy:\", np.mean(pred == y))\n",
    "print(\"Importance:\", feature_importance(rfc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHCKQ4hMJY1w"
   },
   "source": [
    "### Задание 4 (1 балл)\n",
    "Теперь поработаем с реальными данными.\n",
    "\n",
    "Выборка состоит из публичных анонимизированных данных пользователей социальной сети Вконтакте. Первые два столбца отражают возрастную группу (`zoomer`, `doomer` и `boomer`) и пол (`female`, `male`). Все остальные столбцы являются бинарными признаками, каждый из них определяет, подписан ли пользователь на определенную группу/публичную страницу или нет.\\\n",
    "\\\n",
    "Необходимо обучить два классификатора, один из которых определяет возрастную группу, а второй - пол.\\\n",
    "\\\n",
    "Эксперименты с множеством используемых признаков и подбор гиперпараметров приветствуются. Лес должен строиться за какое-то разумное время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mga_WYifJY1w"
   },
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    dataframe = pandas.read_csv(path, header=0)\n",
    "    dataset = dataframe.values.tolist()\n",
    "    random.shuffle(dataset)\n",
    "    y_age = [row[0] for row in dataset]\n",
    "    y_sex = [row[1] for row in dataset]\n",
    "    X = [row[2:] for row in dataset]\n",
    "    \n",
    "    return np.array(X), np.array(y_age), np.array(y_sex), list(dataframe.columns)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qkz8Av6QJY1x"
   },
   "outputs": [],
   "source": [
    "X, y_age, y_sex, features = read_dataset(\"vk.csv\")\n",
    "X_train, X_test, y_age_train, y_age_test, y_sex_train, y_sex_test = train_test_split(X, y_age, y_sex, train_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjUqJL0fJY1x"
   },
   "source": [
    "#### Возраст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wg42KSCoJY1x"
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "rfc.fit(X_train, y_age_train)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X_test) == y_age_test))\n",
    "#print(\"Most important features:\")\n",
    "#for i, name in enumerate(most_important_features(feature_importance(rfc), features, 20)):\n",
    "    #print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sP0pulKJY1x"
   },
   "source": [
    "#### Пол"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUou10q0JY1x"
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=10)\n",
    "rfc.fit(X_train, y_sex_train)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X_test) == y_sex_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(feature_importance(rfc), features, 20)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9Y4RHyhJY1y"
   },
   "source": [
    "### CatBoost\n",
    "В качестве аьтернативы попробуем CatBoost. \n",
    "\n",
    "Устаниовить его можно просто с помощью `pip install catboost`. Туториалы можно найти, например, [здесь](https://catboost.ai/docs/concepts/python-usages-examples.html#multiclassification) и [здесь](https://github.com/catboost/tutorials/blob/master/python_tutorial.ipynb). Главное - не забудьте использовать `loss_function='MultiClass'`.\\\n",
    "\\\n",
    "Сначала протестируйте CatBoost на синтетических данных. Выведите точность и важность признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtMO1OPoJY1y"
   },
   "outputs": [],
   "source": [
    "X, y = synthetic_dataset(1000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
    "\n",
    "classifier = CatBoostClassifier(loss_function='MultiClass')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy:\", np.mean(classifier.predict(X_test) == y))\n",
    "print(\"Importance:\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yS1cvFRHJY1y"
   },
   "source": [
    "### Задание 5 (3 балла)\n",
    "Попробуем применить один из используемых на практике алгоритмов. В этом нам поможет CatBoost. Также, как и реализованный ними RandomForest, применим его для определения пола и возраста пользователей сети Вконтакте, выведите названия наиболее важных признаков так же, как в задании 3.\\\n",
    "\\\n",
    "Эксперименты с множеством используемых признаков и подбор гиперпараметров приветствуются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miOvPcXbJY1y"
   },
   "outputs": [],
   "source": [
    "X, y_age, y_sex, features = read_dataset(\"vk.csv\")\n",
    "X_train, X_test, y_age_train, y_age_test, y_sex_train, y_sex_test = train_test_split(X, y_age, y_sex, train_size=0.9)\n",
    "X_train, X_eval, y_age_train, y_age_eval, y_sex_train, y_sex_eval = train_test_split(X_train, y_age_train, y_sex_train, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVVhaiIxJY1z"
   },
   "source": [
    "#### Возраст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umGP9_rpJY1z"
   },
   "outputs": [],
   "source": [
    "classifier = CatBoostClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy:\", np.mean(classifier.predict(X_test) == y_age_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(None, features, 10)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7eavYC0JY1z"
   },
   "source": [
    "#### Пол"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvJNAmJxJY1z"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", np.mean(None == y_sex_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(None, features, 10)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw09_task.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
